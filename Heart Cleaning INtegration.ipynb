{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "314319e1",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af32d114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
      "0   63    1   3       145   233    1        0      150      0      2.3      0   \n",
      "1   37    1   2       130   250    0        1      187      0      3.5      0   \n",
      "2   41    0   1       130   204    0        0      172      0      1.4      2   \n",
      "3   56    1   1       120   236    0        1      178      0      0.8      2   \n",
      "4   57    0   0       120   354    0        1      163      1      0.6      2   \n",
      "\n",
      "   ca  thal  target  \n",
      "0   0     1       1  \n",
      "1   0     2       1  \n",
      "2   0     2       1  \n",
      "3   0     2       1  \n",
      "4   0     2       1  \n",
      "\n",
      "Missing values in the dataset:\n",
      "age         0\n",
      "sex         0\n",
      "cp          0\n",
      "trestbps    0\n",
      "chol        0\n",
      "fbs         0\n",
      "restecg     0\n",
      "thalach     0\n",
      "exang       0\n",
      "oldpeak     0\n",
      "slope       0\n",
      "ca          0\n",
      "thal        0\n",
      "target      0\n",
      "dtype: int64\n",
      "\n",
      "Cleaned dataset:\n",
      "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
      "0   63    1   3       145   233    1        0      150      0      2.3      0   \n",
      "1   37    1   2       130   250    0        1      187      0      3.5      0   \n",
      "2   41    0   1       130   204    0        0      172      0      1.4      2   \n",
      "3   56    1   1       120   236    0        1      178      0      0.8      2   \n",
      "4   57    0   0       120   354    0        1      163      1      0.6      2   \n",
      "\n",
      "   ca  thal  target  \n",
      "0   0     1       1  \n",
      "1   0     2       1  \n",
      "2   0     2       1  \n",
      "3   0     2       1  \n",
      "4   0     2       1  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the heart dataset\n",
    "heart_data = pd.read_csv('heart.csv')\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(heart_data.head())\n",
    "\n",
    "# Check for missing values\n",
    "missing_values = heart_data.isnull().sum()\n",
    "print(\"\\nMissing values in the dataset:\")\n",
    "print(missing_values)\n",
    "\n",
    "# Remove rows with missing values\n",
    "heart_data_clean = heart_data.dropna()\n",
    "\n",
    "# Check for duplicates and remove them\n",
    "heart_data_clean = heart_data_clean.drop_duplicates()\n",
    "\n",
    "# Display the cleaned dataset\n",
    "print(\"\\nCleaned dataset:\")\n",
    "print(heart_data_clean.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fc639c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "36291d69",
   "metadata": {},
   "source": [
    "### Data Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3a14962",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gayak\\AppData\\Local\\Temp\\ipykernel_99360\\3335742728.py:2: DtypeWarning: Columns (10,14,18) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  air_quality_data = pd.read_csv('air.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               time  station AMB_TEMP  CH4    CO  NMHC   NO NO2 NOx  O3  ...  \\\n",
      "0  2015/01/01 00:00  Banqiao       16  2.1  0.79  0.14  1.2  16  17  37  ...   \n",
      "1  2015/01/01 01:00  Banqiao       16  2.1   0.8  0.15  1.3  16  17  36  ...   \n",
      "2  2015/01/01 02:00  Banqiao       16  2.1  0.71  0.13    1  13  14  38  ...   \n",
      "3  2015/01/01 03:00  Banqiao       15    2  0.66  0.12  0.8  11  12  39  ...   \n",
      "4  2015/01/01 04:00  Banqiao       15    2  0.53  0.11  0.6  10  11  38  ...   \n",
      "\n",
      "  RAINFALL RAIN_COND  RH  SO2  THC UVB WD_HR WIND_DIREC WIND_SPEED WS_HR  \n",
      "0       NR        NR  57   12  2.2   0    69         69        4.7   4.2  \n",
      "1       NR        NR  57   11  2.2   0    67         65          4     4  \n",
      "2       NR        NR  57    8  2.2   0    63         53        3.7   3.5  \n",
      "3       NR        NR  58  6.5  2.2   0    63         63        4.1   3.3  \n",
      "4       NR        NR  58  5.5  2.1   0    69         67          3   3.1  \n",
      "\n",
      "[5 rows x 23 columns]\n",
      "\n",
      "Concatenated dataset:\n",
      "    age  sex   cp  trestbps   chol  fbs  restecg  thalach  exang  oldpeak  \\\n",
      "0  63.0  1.0  3.0     145.0  233.0  1.0      0.0    150.0    0.0      2.3   \n",
      "1  37.0  1.0  2.0     130.0  250.0  0.0      1.0    187.0    0.0      3.5   \n",
      "2  41.0  0.0  1.0     130.0  204.0  0.0      0.0    172.0    0.0      1.4   \n",
      "3  56.0  1.0  1.0     120.0  236.0  0.0      1.0    178.0    0.0      0.8   \n",
      "4  57.0  0.0  0.0     120.0  354.0  0.0      1.0    163.0    1.0      0.6   \n",
      "\n",
      "   ...  RAINFALL  RAIN_COND   RH  SO2  THC  UVB WD_HR WIND_DIREC WIND_SPEED  \\\n",
      "0  ...       NaN        NaN  NaN  NaN  NaN  NaN   NaN        NaN        NaN   \n",
      "1  ...       NaN        NaN  NaN  NaN  NaN  NaN   NaN        NaN        NaN   \n",
      "2  ...       NaN        NaN  NaN  NaN  NaN  NaN   NaN        NaN        NaN   \n",
      "3  ...       NaN        NaN  NaN  NaN  NaN  NaN   NaN        NaN        NaN   \n",
      "4  ...       NaN        NaN  NaN  NaN  NaN  NaN   NaN        NaN        NaN   \n",
      "\n",
      "  WS_HR  \n",
      "0   NaN  \n",
      "1   NaN  \n",
      "2   NaN  \n",
      "3   NaN  \n",
      "4   NaN  \n",
      "\n",
      "[5 rows x 37 columns]\n"
     ]
    }
   ],
   "source": [
    "# Load the air quality dataset\n",
    "air_quality_data = pd.read_csv('air.csv')\n",
    "\n",
    "# Display the first few rows of the air quality dataset\n",
    "print(air_quality_data.head())\n",
    "# Concatenate the datasets vertically\n",
    "merged_data = pd.concat([heart_data_clean, air_quality_data])# ignore_index=True)\n",
    "\n",
    "# Display the first few rows of the concatenated dataset\n",
    "print(\"\\nConcatenated dataset:\")\n",
    "print(merged_data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043b9f9b",
   "metadata": {},
   "source": [
    "### Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2c34635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Column names in the merged_data DataFrame:\n",
      "Index(['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach',\n",
      "       'exang', 'oldpeak', 'slope', 'ca', 'thal', 'target', 'time', 'station',\n",
      "       'AMB_TEMP', 'CH4', 'CO', 'NMHC', 'NO', 'NO2', 'NOx', 'O3', 'PH_RAIN',\n",
      "       'PM10', 'PM2.5', 'RAINFALL', 'RAIN_COND', 'RH', 'SO2', 'THC', 'UVB',\n",
      "       'WD_HR', 'WIND_DIREC', 'WIND_SPEED', 'WS_HR'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Display the column names of merged_data\n",
    "print(\"\\nColumn names in the merged_data DataFrame:\")\n",
    "print(merged_data.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15e358b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns = [\n",
    "    'age',  'sex',   'cp',  'trestbps',   'chol',  'fbs',  'restecg',  'thalach'\n",
    "]\n",
    "categorical_columns = [\n",
    "    'RH', 'WD_HR','WIND_DIREC'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d73f1779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert date/time columns to numeric (UNIX timestamp)\n",
    "for col in numerical_columns:\n",
    "    if merged_data[col].dtype == 'object':  # Check if column is of object type\n",
    "        merged_data[col] = pd.to_datetime(merged_data[col], errors='coerce')\n",
    "        merged_data[col] = merged_data[col].astype('int64')  # Convert to UNIX timestamp\n",
    "\n",
    "# Handle missing values\n",
    "merged_data[numerical_columns] = merged_data[numerical_columns].fillna(merged_data[numerical_columns].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7df7e6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "for col in numerical_columns:\n",
    "    merged_data[col] = pd.to_numeric(merged_data[col], errors='coerce')\n",
    "\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e2dd7f",
   "metadata": {},
   "source": [
    "### Data Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e91fef81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test set: 86.89%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.83      0.86        29\n",
      "           1       0.85      0.91      0.88        32\n",
      "\n",
      "    accuracy                           0.87        61\n",
      "   macro avg       0.87      0.87      0.87        61\n",
      "weighted avg       0.87      0.87      0.87        61\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('heart.csv')  # Replace 'your_dataset.csv' with your dataset file path\n",
    "\n",
    "# Define the features and target variable\n",
    "features = data.drop('target', axis=1)  # Replace 'target' with your target column name\n",
    "target = data['target']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define numerical and categorical columns\n",
    "numerical_columns = X_train.select_dtypes(include=['float64', 'int64']).columns\n",
    "categorical_columns = X_train.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Create a column transformer\n",
    "column_transformer = ColumnTransformer([\n",
    "    ('scaler', StandardScaler(), numerical_columns),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore'), categorical_columns)\n",
    "])\n",
    "\n",
    "# Create a RandomForestClassifier model\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Create a pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', column_transformer),\n",
    "    ('classifier', model)\n",
    "])\n",
    "\n",
    "# Define hyperparameter grid for tuning\n",
    "param_grid = {\n",
    "    'classifier__n_estimators': [100, 200],\n",
    "    'classifier__max_depth': [5, 10, None]\n",
    "}\n",
    "\n",
    "# Perform grid search for hyperparameter tuning\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy on the test set: {accuracy * 100:.2f}%\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Save the model if needed\n",
    "# import joblib\n",
    "# joblib.dump(best_model, 'trained_model.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb0d357",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sure, let's break down the code and explain each part:\n",
    "\n",
    "1. **Importing Libraries**:\n",
    "   ```python\n",
    "   import pandas as pd\n",
    "   ```\n",
    "   This imports the Pandas library with an alias `pd`, which is commonly used for data manipulation and analysis.\n",
    "\n",
    "2. **Loading Data**:\n",
    "   ```python\n",
    "   heart_data = pd.read_csv('heart.csv')\n",
    "   ```\n",
    "   This line reads a CSV file named `'heart.csv'` into a Pandas DataFrame called `heart_data`.\n",
    "\n",
    "3. **Displaying Data**:\n",
    "   ```python\n",
    "   print(heart_data.head())\n",
    "   ```\n",
    "   This line prints the first few rows of the DataFrame `heart_data` to inspect the data.\n",
    "\n",
    "4. **Checking for Missing Values**:\n",
    "   ```python\n",
    "   missing_values = heart_data.isnull().sum()\n",
    "   print(\"\\nMissing values in the dataset:\")\n",
    "   print(missing_values)\n",
    "   ```\n",
    "   This code calculates the sum of missing values in each column of the DataFrame `heart_data` and prints the result.\n",
    "\n",
    "5. **Cleaning Data**:\n",
    "   ```python\n",
    "   heart_data_clean = heart_data.dropna().drop_duplicates()\n",
    "   ```\n",
    "   This line removes rows with missing values and duplicates from the DataFrame `heart_data` and assigns the cleaned data to `heart_data_clean`.\n",
    "\n",
    "6. **Loading Another Dataset**:\n",
    "   ```python\n",
    "   air_quality_data = pd.read_csv('air.csv')\n",
    "   ```\n",
    "   This code reads another CSV file named `'air.csv'` into a new DataFrame called `air_quality_data`.\n",
    "\n",
    "7. **Concatenating Data**:\n",
    "   ```python\n",
    "   merged_data = pd.concat([heart_data_clean, air_quality_data])\n",
    "   ```\n",
    "   This line concatenates the `heart_data_clean` DataFrame and `air_quality_data` DataFrame vertically (row-wise).\n",
    "\n",
    "8. **Displaying Concatenated Data**:\n",
    "   ```python\n",
    "   print(\"\\nConcatenated dataset:\")\n",
    "   print(merged_data.head())\n",
    "   ```\n",
    "   This code prints the first few rows of the concatenated DataFrame `merged_data`.\n",
    "\n",
    "9. **Handling Numerical Columns**:\n",
    "   ```python\n",
    "   for col in numerical_columns:\n",
    "       merged_data[col] = pd.to_numeric(merged_data[col], errors='coerce')\n",
    "   ```\n",
    "   This loop iterates over numerical columns and converts their data type to numeric, handling any errors by coercing invalid parsing to NaN.\n",
    "\n",
    "10. **One-Hot Encoding**:\n",
    "    ```python\n",
    "    encoder = OneHotEncoder(sparse_output=False)\n",
    "    ```\n",
    "    This line initializes a OneHotEncoder object with the parameter `sparse_output=False`, which means it will return a dense array instead of a sparse matrix.\n",
    "\n",
    "11. **Importing Libraries for Machine Learning**:\n",
    "    ```python\n",
    "    from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.metrics import accuracy_score, classification_report\n",
    "    from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "    from sklearn.compose import ColumnTransformer\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    ```\n",
    "    These lines import various modules from scikit-learn (`sklearn`) library for machine learning tasks such as data preprocessing, model selection, and evaluation.\n",
    "\n",
    "12. **Splitting Data for Training and Testing**:\n",
    "    ```python\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
    "    ```\n",
    "    This code splits the data into training and testing sets using `train_test_split` function from scikit-learn.\n",
    "\n",
    "13. **Creating a Pipeline**:\n",
    "    ```python\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessor', column_transformer),\n",
    "        ('classifier', model)\n",
    "    ])\n",
    "    ```\n",
    "    This line creates a pipeline that includes preprocessing steps (scaling and encoding) and a machine learning model (RandomForestClassifier).\n",
    "\n",
    "14. **Hyperparameter Tuning**:\n",
    "    ```python\n",
    "    grid_search = GridSearchCV(pipeline, param_grid, cv=5)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    ```\n",
    "    This code performs grid search cross-validation to find the best hyperparameters for the RandomForestClassifier model.\n",
    "\n",
    "15. **Evaluation**:\n",
    "    ```python\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    ```\n",
    "    This code evaluates the best model on the test set and calculates the accuracy score.\n",
    "\n",
    "16. **Printing Results**:\n",
    "    ```python\n",
    "    print(f\"Accuracy on the test set: {accuracy * 100:.2f}%\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    ```\n",
    "    These lines print the accuracy score and classification report of the model on the test set.\n",
    "\n",
    "This code essentially loads two datasets, cleans them, concatenates them, preprocesses the data, builds a machine learning pipeline, tunes hyperparameters, and evaluates the model's performance. It's a comprehensive example of a typical machine learning workflow using Python and scikit-learn."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
